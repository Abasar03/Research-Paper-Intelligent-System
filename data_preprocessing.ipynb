{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8tU0ZirbK9ha",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tU0ZirbK9ha",
        "outputId": "c35da1b0-434f-4a9f-9641-b0ff1a5038d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "/usr/local/lib/python3.12/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ExuBzkObUNmqgmeaVMF-OqnAuo9yuozK\n",
            "From (redirected): https://drive.google.com/uc?id=1ExuBzkObUNmqgmeaVMF-OqnAuo9yuozK&confirm=t&uuid=01aeb256-2397-4eec-bfdf-0452590dca95\n",
            "To: /content/all_capped_keywords.zip\n",
            "100% 579M/579M [00:09<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1ExuBzkObUNmqgmeaVMF-OqnAuo9yuozK -O all_capped_keywords.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vINdcz5xLn0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vINdcz5xLn0a",
        "outputId": "80cd3cd0-bca8-436a-8e2b-7e90cf4b744f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  all_capped_keywords.zip\n",
            "  inflating: cs_papersum_keywords/All_capped_keywords.csv  \n",
            "total 3000408\n",
            "drwxr-xr-x 2 root root       4096 Sep 20 13:28 .\n",
            "drwxr-xr-x 1 root root       4096 Sep 20 13:28 ..\n",
            "-rw-r--r-- 1 root root 3072402092 Jul 29  2024 All_capped_keywords.csv\n"
          ]
        }
      ],
      "source": [
        "!unzip all_capped_keywords.zip -d cs_papersum_keywords\n",
        "!ls -la cs_papersum_keywords/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qSfZ0ID4N0dr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSfZ0ID4N0dr",
        "outputId": "e5cc052a-4f57-4d83-a41c-83fee5c94b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (91919, 25)\n",
            "Rows: 91919, Columns: 25\n",
            "\n",
            "Column names:\n",
            "['paperID', 'venue', 'year', 'openAccessPdf', 'url', 'authors', 'referenceCount', 'title', 'abstract', 'conclusion', 'Chatgpt Response', 'Key Takeaways', 'Importance', 'Model/Method Proposed', 'Performance', 'Effectiveness', 'Future Works', 'Sentiment', 'Sentiment Score', 'combined', 'combined_keywords', 'response_keywords', 'future_work_keywords', 'capped_keywords', 'field']\n",
            "\n",
            "First 5 rows:\n",
            "                                    paperID  \\\n",
            "0  000194903cb83bd4af714f950d0266382e2772fc   \n",
            "1  000351d89ee73f2fa721961129ec9c0758cf20ca   \n",
            "2  001198d3ef0d304718cfd64ff241d0133adcf928   \n",
            "3  00133d41d5ecef1a9d046d2b92bb2a23a335cb7c   \n",
            "4  002285ade41b9e1313f9c914b99e68b62fab7ff1   \n",
            "\n",
            "                                        venue  year  openAccessPdf  \\\n",
            "0  AAAI Conference on Artificial Intelligence  2021           True   \n",
            "1  AAAI Conference on Artificial Intelligence  2017          False   \n",
            "2  AAAI Conference on Artificial Intelligence  2017           True   \n",
            "3  AAAI Conference on Artificial Intelligence  2020           True   \n",
            "4  AAAI Conference on Artificial Intelligence  2017           True   \n",
            "\n",
            "                                                 url  \\\n",
            "0  https://ojs.aaai.org/index.php/AAAI/article/do...   \n",
            "1                                                NaN   \n",
            "2  https://ojs.aaai.org/index.php/AAAI/article/do...   \n",
            "3  https://ojs.aaai.org/index.php/AAAI/article/do...   \n",
            "4  https://aaai.org/ojs/index.php/AAAI/article/do...   \n",
            "\n",
            "                                             authors  referenceCount  \\\n",
            "0  Zhihong Chen (2144309151),Taiping Yao (5130095...              35   \n",
            "1  Elias FernÃ¡ndez Domingos (9563986),J. C. Burgu...               9   \n",
            "2  Cao Xiao (145781464),Ping Zhang (2154286366),W...              29   \n",
            "3  Yanchao Sun (120738683),Xiangyu Yin (31186224)...              37   \n",
            "4  M. Riemer (40497459),Tim Klinger (2652016),Dja...              62   \n",
            "\n",
            "                                               title  \\\n",
            "0  Generalizable Representation Learning for Mixt...   \n",
            "1  Coordinating Human and Agent Behavior in Colle...   \n",
            "2  Adverse Drug Reaction Prediction with Symbolic...   \n",
            "3  TempLe: Learning Template of Transitions for S...   \n",
            "4  Scalable Recollections for Continual Lifelong ...   \n",
            "\n",
            "                                            abstract  \\\n",
            "0  Face anti-spoofing approach based on domain ge...   \n",
            "1  \\n \\n Various social situations entail a colle...   \n",
            "2  \\n \\n Adverse drug reaction (ADR) is a major b...   \n",
            "3  Transferring knowledge among various environme...   \n",
            "4  Given the recent success of Deep Learning appl...   \n",
            "\n",
            "                                          conclusion  ...  \\\n",
            "0  conclusion:As shown in Figure 3, Tables 1 and2...  ...   \n",
            "1                                                NaN  ...   \n",
            "2  conclusion:In this paper, we presented three L...  ...   \n",
            "3  conclusion:In this work, we propose TempLe, th...  ...   \n",
            "4                                                NaN  ...   \n",
            "\n",
            "                                       Effectiveness  \\\n",
            "0                               ['Highly effective']   \n",
            "1                           ['Moderately effective']   \n",
            "2  ['Highly effective in improving prediction acc...   \n",
            "3                               ['Highly effective']   \n",
            "4                           ['Moderately effective']   \n",
            "\n",
            "                                        Future Works  \\\n",
            "0  ['Further enhance interpretability and visuali...   \n",
            "1  ['Explore further applications of anticipatory...   \n",
            "2  ['Improvement of the medical ontology system w...   \n",
            "3  ['Investigating transition probability and rew...   \n",
            "4  ['Exploring further improvements in training e...   \n",
            "\n",
            "                                           Sentiment Sentiment Score  \\\n",
            "0                                       ['Positive']         ['8.5']   \n",
            "1  ['The use of anticipative agents and recurrent...           ['8']   \n",
            "2  ['The proposed models show promise in enhancin...         ['8.5']   \n",
            "3                                       ['Positive']         ['8.5']   \n",
            "4                                       ['Positive']           ['8']   \n",
            "\n",
            "                                            combined  \\\n",
            "0  Generalizable Representation Learning for Mixt...   \n",
            "1  Coordinating Human and Agent Behavior in Colle...   \n",
            "2  Adverse Drug Reaction Prediction with Symbolic...   \n",
            "3  TempLe: Learning Template of Transitions for S...   \n",
            "4  Scalable Recollections for Continual Lifelong ...   \n",
            "\n",
            "                                   combined_keywords  \\\n",
            "0  [('spoofing meta learning', 0.6263), ('face an...   \n",
            "1  [('collective risk game', 0.5749), ('agent beh...   \n",
            "2  [('drug reaction prediction', 0.5413), ('adr p...   \n",
            "3  [('learning multiple tasks', 0.5994), ('multi ...   \n",
            "4  [('continual lifelong learning', 0.7155), ('re...   \n",
            "\n",
            "                                   response_keywords  \\\n",
            "0  [('meta learning domain', 0.5346), ('face anti...   \n",
            "1  [('collective risk game', 0.5331), ('agents co...   \n",
            "2  [('drug adr data', 0.5261), ('lda adr predicti...   \n",
            "3  [('learns multiple tasks', 0.6435), ('learning...   \n",
            "4  [('continual lifelong learning', 0.6788), ('li...   \n",
            "\n",
            "                                future_work_keywords  \\\n",
            "0  [('face anti spoofing', 0.6095), ('anti spoofi...   \n",
            "1  [('applications anticipatory behavior', 0.6232...   \n",
            "2  [('improvement medical ontology', 0.7167), ('m...   \n",
            "3  [('similarities continuous mdp', 0.551), ('mod...   \n",
            "4  [('exploring improvements training', 0.6438), ...   \n",
            "\n",
            "                                     capped_keywords                    field  \n",
            "0  [('spoofing meta learning', 0.6263), ('face an...  Artificial Intelligence  \n",
            "1  [('collective risk game', 0.5749), ('agent beh...  Artificial Intelligence  \n",
            "2  [('drug reaction prediction', 0.5413), ('adr p...  Artificial Intelligence  \n",
            "3  [('learning multiple tasks', 0.5994), ('multi ...  Artificial Intelligence  \n",
            "4  [('continual lifelong learning', 0.7155), ('re...  Artificial Intelligence  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "\n",
            "Last 5 rows:\n",
            "                                        paperID               venue  year  \\\n",
            "91914  ffc9f61c8897bc22778b97ef1834b8e53337066b  The Web Conference  2017   \n",
            "91915  ffe8738f6c0d232e73de3680cec36365903e4fdf  The Web Conference  2019   \n",
            "91916  fff69efd4a76f952c61a8ff38d81aca9d2aef4fe  The Web Conference  2020   \n",
            "91917  fff6db1f8680c2eb1011e13b5dac946c2bc93258  The Web Conference  2021   \n",
            "91918  fffbbc9d515ca6b5e9bc59cdded8c96a90c364d3  The Web Conference  2020   \n",
            "\n",
            "       openAccessPdf                                                url  \\\n",
            "91914          False                                                NaN   \n",
            "91915          False                                                NaN   \n",
            "91916          False                                                NaN   \n",
            "91917           True  https://dl.acm.org/doi/pdf/10.1145/3442381.345...   \n",
            "91918           True                   https://arxiv.org/pdf/2002.01633   \n",
            "\n",
            "                                                 authors  referenceCount  \\\n",
            "91914  B. Kveton (1681967),S. Muthukrishnan (14496353...              15   \n",
            "91915                             Mengfan Yao (32857558)              18   \n",
            "91916  Shuangyong Song (3197458),Chao Wang (214444686...               4   \n",
            "91917  Ines Arous (115651840),Jie Yang (2118579490),M...              46   \n",
            "91918  Deyu Bo (1491450638),Xiao Wang (2118449003),C....              33   \n",
            "\n",
            "                                                   title  \\\n",
            "91914      Finding Subcube Heavy Hitters in Data Streams   \n",
            "91915  Robust Detection of Cyberbullying in Social Media   \n",
            "91916  Supervised Term Weight Training for Improving ...   \n",
            "91917  Peer Grading the Peer Reviews: A Dual-Role App...   \n",
            "91918                 Structural Deep Clustering Network   \n",
            "\n",
            "                                                abstract  \\\n",
            "91914  Data streams typically have items of large num...   \n",
            "91915  The potentially detrimental effects of cyberbu...   \n",
            "91916  We design several supervised methods of traini...   \n",
            "91917  Scientific peer review is pivotal to maintain ...   \n",
            "91918  Clustering is a fundamental task in data analy...   \n",
            "\n",
            "                                              conclusion  ...  \\\n",
            "91914                                                NaN  ...   \n",
            "91915                                                NaN  ...   \n",
            "91916                                                NaN  ...   \n",
            "91917                                                NaN  ...   \n",
            "91918  conclusion:In this paper, we make the first at...  ...   \n",
            "\n",
            "                  Effectiveness  \\\n",
            "91914  ['Moderately effective']   \n",
            "91915  ['Moderately effective']   \n",
            "91916  ['Moderately effective']   \n",
            "91917      ['Highly effective']   \n",
            "91918     ['Highly effective.']   \n",
            "\n",
            "                                            Future Works  \\\n",
            "91914  ['Explore further the model-based data stream ...   \n",
            "91915  ['Further refinement of the sequential hypothe...   \n",
            "91916  ['Exploration of deep learning models for matc...   \n",
            "91917  ['Explore ways to further automate the review ...   \n",
            "91918  ['Exploring further enhancements to the integr...   \n",
            "\n",
            "                                               Sentiment Sentiment Score  \\\n",
            "91914  ['The model-based approach provides a promisin...           ['8']   \n",
            "91915                                       ['Positive']           ['8']   \n",
            "91916                                        ['Neutral']           ['5']   \n",
            "91917  ['Positive sentiment towards the proposed appr...         ['8.5']   \n",
            "91918  ['Positive sentiment towards the effectiveness...          ['9.']   \n",
            "\n",
            "                                                combined  \\\n",
            "91914  Finding Subcube Heavy Hitters in Data Streams ...   \n",
            "91915  Robust Detection of Cyberbullying in Social Me...   \n",
            "91916  Supervised Term Weight Training for Improving ...   \n",
            "91917  Peer Grading the Peer Reviews: A Dual-Role App...   \n",
            "91918  Structural Deep Clustering Network Clustering ...   \n",
            "\n",
            "                                       combined_keywords  \\\n",
            "91914  [('heavy hitters query', 0.5082), ('heavy hitt...   \n",
            "91915  [('classification accuracy cyberbullying', 0.7...   \n",
            "91916  [('knowledge matching chatbots', 0.6967), ('qu...   \n",
            "91917  [('grading peer reviews', 0.6251), ('reviews p...   \n",
            "91918  [('structural deep clustering', 0.668), ('deep...   \n",
            "\n",
            "                                       response_keywords  \\\n",
            "91914  [('heavy hitters query', 0.6276), ('subcube he...   \n",
            "91915  [('cyberbullying detection sentiment', 0.7136)...   \n",
            "91916  [('knowledge matching chatbots', 0.6972), ('kn...   \n",
            "91917  [('review conformity peer', 0.6531), ('peer re...   \n",
            "91918  [('deep clustering network', 0.7104), ('deep c...   \n",
            "\n",
            "                                    future_work_keywords  \\\n",
            "91914  [('data streams', 0.6436), ('dimensions data s...   \n",
            "91915  [('semisupervised methods cyberbullying', 0.76...   \n",
            "91916  [('question knowledge matching', 0.6986), ('te...   \n",
            "91917  [('approach peer review', 0.7028), ('peer revi...   \n",
            "91918  [('exploring enhancements integration', 0.5325...   \n",
            "\n",
            "                                         capped_keywords  \\\n",
            "91914  [('heavy hitters query', 0.5082), ('heavy hitt...   \n",
            "91915  [('classification accuracy cyberbullying', 0.7...   \n",
            "91916  [('knowledge matching chatbots', 0.6967), ('qu...   \n",
            "91917  [('grading peer reviews', 0.6251), ('reviews p...   \n",
            "91918  [('structural deep clustering', 0.668), ('deep...   \n",
            "\n",
            "                                    field  \n",
            "91914  Computer Networks & Communications  \n",
            "91915  Computer Networks & Communications  \n",
            "91916  Computer Networks & Communications  \n",
            "91917  Computer Networks & Communications  \n",
            "91918  Computer Networks & Communications  \n",
            "\n",
            "[5 rows x 25 columns]\n",
            "\n",
            "Data types:\n",
            "paperID                  object\n",
            "venue                    object\n",
            "year                      int64\n",
            "openAccessPdf              bool\n",
            "url                      object\n",
            "authors                  object\n",
            "referenceCount            int64\n",
            "title                    object\n",
            "abstract                 object\n",
            "conclusion               object\n",
            "Chatgpt Response         object\n",
            "Key Takeaways            object\n",
            "Importance               object\n",
            "Model/Method Proposed    object\n",
            "Performance              object\n",
            "Effectiveness            object\n",
            "Future Works             object\n",
            "Sentiment                object\n",
            "Sentiment Score          object\n",
            "combined                 object\n",
            "combined_keywords        object\n",
            "response_keywords        object\n",
            "future_work_keywords     object\n",
            "capped_keywords          object\n",
            "field                    object\n",
            "dtype: object\n",
            "\n",
            "Missing values:\n",
            "paperID                      0\n",
            "venue                        0\n",
            "year                         0\n",
            "openAccessPdf                0\n",
            "url                      41880\n",
            "authors                    595\n",
            "referenceCount               0\n",
            "title                        0\n",
            "abstract                  4643\n",
            "conclusion               52888\n",
            "Chatgpt Response             0\n",
            "Key Takeaways                0\n",
            "Importance                   0\n",
            "Model/Method Proposed        0\n",
            "Performance                  0\n",
            "Effectiveness                0\n",
            "Future Works                 0\n",
            "Sentiment                    0\n",
            "Sentiment Score              0\n",
            "combined                     0\n",
            "combined_keywords            0\n",
            "response_keywords            0\n",
            "future_work_keywords         0\n",
            "capped_keywords              0\n",
            "field                     7111\n",
            "dtype: int64\n",
            "\n",
            "Basic statistics:\n",
            "               year  referenceCount\n",
            "count  91919.000000    91919.000000\n",
            "mean    2020.524875       42.063687\n",
            "std        1.936172       25.554432\n",
            "min     2017.000000        0.000000\n",
            "25%     2019.000000       26.000000\n",
            "50%     2021.000000       40.000000\n",
            "75%     2022.000000       55.000000\n",
            "max     2024.000000      767.000000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('cs_papersum_keywords/All_capped_keywords.csv')\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
        "\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nLast 5 rows:\")\n",
        "print(df.tail())\n",
        "\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cQqNk4UcPvib",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQqNk4UcPvib",
        "outputId": "60cb14f7-b9e8-4883-f461-644c65050387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Research fields distribution:\n",
            "field\n",
            "Computational Theory                  17214\n",
            "Computer Vision                       14842\n",
            "Natural Language Processing           14131\n",
            "Artificial Intelligence               12084\n",
            "Human Computer Interaction             7393\n",
            "Computing in Biomedical Fields         6310\n",
            "Computer Networks & Communications     3900\n",
            "Graphics and Computer-Aided Design     3155\n",
            "Software Engineering                   2482\n",
            "Computer Security & Cryptography       1341\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Year distribution:\n",
            "year\n",
            "2017     8120\n",
            "2018     8704\n",
            "2019    12642\n",
            "2020    13130\n",
            "2021    15491\n",
            "2022    14851\n",
            "2023    18461\n",
            "2024      520\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top venues:\n",
            "venue\n",
            "Neural Information Processing Systems                                                     13857\n",
            "AAAI Conference on Artificial Intelligence                                                10498\n",
            "Computer Vision and Pattern Recognition                                                    9419\n",
            "Conference on Empirical Methods in Natural Language Processing                             7915\n",
            "Annual Meeting of the Association for Computational Linguistics                            6216\n",
            "IEEE International Conference on Computer Vision                                           5423\n",
            "International Conference on Human Factors in Computing Systems                             5048\n",
            "The Web Conference                                                                         3900\n",
            "International Conference on Medical Image Computing and Computer-Assisted Intervention     3591\n",
            "International Conference on Information and Knowledge Management                           3514\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Important columns for research gap analysis\n",
        "key_columns = ['title', 'abstract', 'Future Works', 'field', 'year', 'venue']\n",
        "\n",
        "# Checking unique research fields\n",
        "print(\"\\nResearch fields distribution:\")\n",
        "print(df['field'].value_counts().head(10))\n",
        "\n",
        "# Checking year distribution\n",
        "print(\"\\nYear distribution:\")\n",
        "print(df['year'].value_counts().sort_index().tail(10))\n",
        "\n",
        "# Checking venues\n",
        "print(\"\\nTop venues:\")\n",
        "print(df['venue'].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xkwz0CrDlnTx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkwz0CrDlnTx",
        "outputId": "4302f85c-70dc-4fdb-e6f0-6dcd9235eedf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before cleaning: 91919\n",
            "After removing missing abstracts: 87276\n"
          ]
        }
      ],
      "source": [
        "# Making a copy\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Removing rows with missing abstracts\n",
        "print(\"Before cleaning:\", len(df_clean))\n",
        "df_clean = df_clean.dropna(subset=['abstract'])\n",
        "print(\"After removing missing abstracts:\", len(df_clean))\n",
        "\n",
        "# Replacing missing values\n",
        "# print(\"Before cleaning:\", len(df_clean))\n",
        "# df[\"authors\"] = df_clean[\"authors\"].fillna(\"Unknown\")\n",
        "# df[\"field\"] = df_clean[\"field\"].fillna(\"Unknown\")\n",
        "# df[\"conclusion\"] = df_clean[\"conclusion\"].fillna(\"\")\n",
        "# print(\"After filling missing values:\", len(df_clean))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DwtFXbSK5D0T",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwtFXbSK5D0T",
        "outputId": "ae974bd5-c437-4b97-b6ba-dbf87eb17300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample Future Works entries:\n",
            "[\"['Further enhance interpretability and visualization', 'Explore applications to other domains beyond face anti-spoofing']\"\n",
            " \"['Explore further applications of anticipatory behavior in complex social scenarios.', 'Investigate the impact of different network architectures on agent decision-making.']\"\n",
            " \"['Improvement of the medical ontology system with real-world evidence.', 'Addressing issues related to false-negative predictions.']\"]\n",
            "\n",
            "Future work keywords sample:\n",
            "[\"[('face anti spoofing', 0.6095), ('anti spoofing', 0.5719), ('spoofing', 0.5294), ('domains face anti', 0.5041), ('applications domains face', 0.4914), ('applications', 0.4733), ('domains face', 0.4467), ('explore applications', 0.3684), ('enhance interpretability visualization', 0.3626), ('face anti', 0.3612), ('applications domains', 0.3538), ('explore applications domains', 0.3494), ('enhance interpretability', 0.3429), ('interpretability visualization', 0.3329), ('interpretability visualization explore', 0.3328), ('visualization explore applications', 0.3267), ('enhance', 0.3254), ('face', 0.3114), ('interpretability', 0.2784), ('domains', 0.2657), ('visualization', 0.256), ('visualization explore', 0.2516), ('anti', 0.2156), ('explore', 0.2017)]\"\n",
            " \"[('applications anticipatory behavior', 0.6232), ('anticipatory behavior complex', 0.5502), ('agent decision making', 0.5474), ('explore applications anticipatory', 0.5403), ('complex social scenarios', 0.5383), ('social scenarios investigate', 0.5253), ('anticipatory behavior', 0.5221), ('behavior complex social', 0.5171), ('behavior complex', 0.5143), ('network architectures agent', 0.4916), ('applications anticipatory', 0.4846), ('social scenarios', 0.4806), ('behavior', 0.4788), ('architectures agent', 0.4682), ('architectures agent decision', 0.467), ('agent decision', 0.4631), ('scenarios investigate', 0.4289), ('architectures', 0.4239), ('applications', 0.423), ('complex social', 0.4208), ('scenarios investigate impact', 0.412), ('explore applications', 0.3944), ('network architectures', 0.3918), ('decision making', 0.3885), ('investigate impact', 0.3793), ('agent', 0.3691), ('social', 0.3483), ('investigate impact different', 0.3404), ('scenarios', 0.3229), ('impact different network', 0.3195), ('investigate', 0.3158), ('network', 0.2786), ('anticipatory', 0.2688), ('different network architectures', 0.2622), ('explore', 0.2498), ('complex', 0.2491), ('impact', 0.1917), ('decision', 0.1858), ('impact different', 0.1649), ('different network', 0.1295), ('making', 0.1067), ('different', 0.0072)]\"\n",
            " \"[('improvement medical ontology', 0.7167), ('medical ontology', 0.6802), ('medical ontology real', 0.64), ('ontology', 0.5714), ('ontology real world', 0.5198), ('ontology real', 0.4652), ('evidence addressing issues', 0.4632), ('addressing issues related', 0.4551), ('medical', 0.4267), ('addressing issues', 0.4159), ('improvement medical', 0.4138), ('evidence addressing', 0.3823), ('issues related', 0.3609), ('world evidence addressing', 0.3264), ('improvement', 0.3257), ('issues', 0.3072), ('evidence', 0.2875), ('issues related false', 0.27), ('real world evidence', 0.2623), ('world evidence', 0.236), ('false negative predictions', 0.228), ('addressing', 0.2275), ('negative predictions', 0.201), ('false', 0.1955), ('false negative', 0.1814), ('related false negative', 0.1675), ('related false', 0.157), ('related', 0.1527), ('predictions', 0.1407), ('real world', 0.1244), ('negative', 0.112), ('world', 0.1113), ('real', 0.0098)]\"]\n"
          ]
        }
      ],
      "source": [
        "# Key columns for research gap analysis\n",
        "research_gap_columns = [\n",
        "    'title', 'abstract', 'Future Works', 'field',\n",
        "    'Model/Method Proposed', 'Performance', 'Effectiveness',\n",
        "    'future_work_keywords', 'capped_keywords'\n",
        "]\n",
        "\n",
        "# Dataset for research gap analysis\n",
        "gap_analysis_df = df_clean[research_gap_columns].copy()\n",
        "\n",
        "# Examining future works column\n",
        "print(\"\\nSample Future Works entries:\")\n",
        "print(df_clean['Future Works'].head(3).values)\n",
        "\n",
        "print(\"\\nFuture work keywords sample:\")\n",
        "print(df_clean['future_work_keywords'].head(3).values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vsOhh20M7IC5",
      "metadata": {
        "id": "vsOhh20M7IC5"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # Converting to owercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Removing newlines/tabs\n",
        "    text = text.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
        "\n",
        "    # Removing extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Removing references\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
        "    text = re.sub(r'\\([^)]*et al\\.,\\s*\\d{4}\\)', '', text)\n",
        "    return text\n",
        "\n",
        "df_clean['future_work_cleaned'] = df_clean['Future Works'].apply(normalize_text)\n",
        "df_clean['abstract_cleaned'] = df_clean['abstract'].apply(normalize_text)\n",
        "df_clean['method_proposed_cleaned'] = df_clean['Model/Method Proposed'].apply(normalize_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_tgz-8amBfmJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tgz-8amBfmJ",
        "outputId": "c6d0c55b-11f1-439c-9407-4cb0e657c5ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conclusion availability:\n",
            "Total papers: 87276\n",
            "Papers with conclusion: 39011\n",
            "Papers without conclusion: 48265\n"
          ]
        }
      ],
      "source": [
        "# Checking what we have for conclusion\n",
        "print(\"Conclusion availability:\")\n",
        "print(f\"Total papers: {len(df_clean)}\")\n",
        "print(f\"Papers with conclusion: {df_clean['conclusion'].notna().sum()}\")\n",
        "print(f\"Papers without conclusion: {df_clean['conclusion'].isna().sum()}\")\n",
        "\n",
        "# Cleaning conclusion where available\n",
        "df_clean['conclusion_cleaned'] = df_clean['conclusion'].apply(normalize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OKN7ooFpBj8k",
      "metadata": {
        "id": "OKN7ooFpBj8k"
      },
      "outputs": [],
      "source": [
        "# Combining text sections to make a research gap corpus\n",
        "def create_gap_corpus(row):\n",
        "    sections = []\n",
        "\n",
        "    # Abstract\n",
        "    if row['abstract_cleaned'] and len(row['abstract_cleaned']) > 20:\n",
        "        sections.append(f\"[ABSTRACT] {row['abstract_cleaned']}\")\n",
        "\n",
        "    # Method/approach\n",
        "    if row['method_proposed_cleaned'] and len(row['method_proposed_cleaned']) > 20:\n",
        "        sections.append(f\"[METHOD] {row['method_proposed_cleaned']}\")\n",
        "\n",
        "    # Future works\n",
        "    if row['future_work_cleaned'] and len(row['future_work_cleaned']) > 20:\n",
        "        sections.append(f\"[FUTURE_WORK] {row['future_work_cleaned']}\")\n",
        "\n",
        "    # Conclusion\n",
        "    if row['conclusion_cleaned'] and len(row['conclusion_cleaned']) > 20:\n",
        "        sections.append(f\"[CONCLUSION] {row['conclusion_cleaned']}\")\n",
        "\n",
        "    return \" \".join(sections)\n",
        "\n",
        "# Creating the gap corpus\n",
        "df_clean['gap_corpus'] = df_clean.apply(create_gap_corpus, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uBw45b31Bsnz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBw45b31Bsnz",
        "outputId": "6ec649d9-98a2-465a-e4a3-4b36d7cdfb4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gap Corpus Statistics:\n",
            "Papers with gap corpus: 87276\n",
            "Average gap corpus length: 2985 characters\n",
            "\n",
            "Sample Gap Corpus Entries\n",
            "\n",
            "Paper 1:\n",
            "Title: Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing\n",
            "Field: Artificial Intelligence\n",
            "Gap Corpus (first 300 chars): [ABSTRACT] face anti-spoofing approach based on domain generalization (dg) has drawn growing attention due to its robustness for unseen scenarios. existing dg methods assume that the domain label is known. however, in real-world applications, the collected dataset always contains mixture domains, wh...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Paper 2:\n",
            "Title: Coordinating Human and Agent Behavior in Collective-Risk Scenarios\n",
            "Field: Artificial Intelligence\n",
            "Gap Corpus (first 300 chars): [ABSTRACT] various social situations entail a collective risk. a well-known example is climate change, wherein the risk of a future environmental disaster clashes with the immediate economic interest of developed and developing countries. the collective-risk game operationalizes this kind of situati...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "Paper 3:\n",
            "Title: Adverse Drug Reaction Prediction with Symbolic Latent Dirichlet Allocation\n",
            "Field: Artificial Intelligence\n",
            "Gap Corpus (first 300 chars): [ABSTRACT] adverse drug reaction (adr) is a major burden for patients and healthcare industry. it usually causes preventable hospitalizations and deaths, while associated with a huge amount of cost. traditional preclinical in vitro safety profiling and clinical safety trials are restricted in terms ...\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Checking the quality of gap corpus\n",
        "print(\"Gap Corpus Statistics:\")\n",
        "print(f\"Papers with gap corpus: {(df_clean['gap_corpus'].str.len() > 0).sum()}\")\n",
        "print(f\"Average gap corpus length: {df_clean['gap_corpus'].str.len().mean():.0f} characters\")\n",
        "\n",
        "# Show examples\n",
        "print(\"\\nSample Gap Corpus Entries\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nPaper {i+1}:\")\n",
        "    print(f\"Title: {df_clean.iloc[i]['title']}\")\n",
        "    print(f\"Field: {df_clean.iloc[i]['field']}\")\n",
        "    print(f\"Gap Corpus (first 300 chars): {df_clean.iloc[i]['gap_corpus'][:300]}...\")\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6sKpXBDB28x",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6sKpXBDB28x",
        "outputId": "8f8b41f4-85c8-439d-fd90-6b986782380a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Papers ready for gap analysis: 80318\n",
            "Original dataset: 87276\n",
            "Filtered out: 6958 papers\n"
          ]
        }
      ],
      "source": [
        "# Filtering papers with substantial gap information\n",
        "min_corpus_length = 100  # Minimum characters for meaningful analysis\n",
        "\n",
        "df_gap_ready = df_clean[\n",
        "    (df_clean['gap_corpus'].str.len() >= min_corpus_length) &\n",
        "    (df_clean['field'].notna())\n",
        "].copy()\n",
        "\n",
        "print(f\"Papers ready for gap analysis: {len(df_gap_ready)}\")\n",
        "print(f\"Original dataset: {len(df_clean)}\")\n",
        "print(f\"Filtered out: {len(df_clean) - len(df_gap_ready)} papers\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j1gHov8TB-xx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1gHov8TB-xx",
        "outputId": "430f570d-92ac-46c3-9742-6a4c5b435c41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top research fields with gap corpus:\n",
            "field\n",
            "Computational Theory                  16103\n",
            "Computer Vision                       14786\n",
            "Natural Language Processing           14007\n",
            "Artificial Intelligence               11930\n",
            "Human Computer Interaction             6893\n",
            "Computing in Biomedical Fields         4579\n",
            "Computer Networks & Communications     3872\n",
            "Graphics and Computer-Aided Design     3050\n",
            "Software Engineering                   2336\n",
            "Computer Hardware & Architecture       1214\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample Gap Corpus from Computational Theory\n",
            "Title: Bandit algorithms: Letting go of logarithmic regret for statistical robustness...\n",
            "Gap Corpus: [ABSTRACT] we study regret minimization in a stochastic multi-armed bandit setting and establish a fundamental trade-off between the regret suffered under an algorithm, and its statistical robustness. considering broad classes of underlying arms' distributions, we show that bandit learning algorithms with logarithmic regret are always inconsistent and that consistent learning algorithms always suf...\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Title: Zeroth-Order Online Alternating Direction Method of Multipliers: Convergence Analysis and Applicatio...\n",
            "Gap Corpus: [ABSTRACT] in this paper, we design and analyze a new zeroth-order online algorithm, namely, the zeroth-order online alternating direction method of multipliers (zoo-admm), which enjoys dual advantages of being gradient-free operation and employing the admm to accommodate complex structured regularizers. compared to the first-order gradient-based online algorithm, we show that zoo-admm requires $\\...\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Seeing gap corpus distribution by field\n",
        "field_counts = df_gap_ready['field'].value_counts().head(10)\n",
        "print(\"Top research fields with gap corpus:\")\n",
        "print(field_counts)\n",
        "\n",
        "# Viewing sample from top field\n",
        "top_field = field_counts.index[0]\n",
        "sample_papers = df_gap_ready[df_gap_ready['field'] == top_field].head(2)\n",
        "\n",
        "print(f\"\\nSample Gap Corpus from {top_field}\")\n",
        "for idx, row in sample_papers.iterrows():\n",
        "    print(f\"Title: {row['title'][:100]}...\")\n",
        "    print(f\"Gap Corpus: {row['gap_corpus'][:400]}...\")\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y2FzWe735IV6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2FzWe735IV6",
        "outputId": "f57f4a6e-f2fb-4777-9a19-d1e942650a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'investigate' appears 14406 times\n",
            "'enhance' appears 13330 times\n",
            "'improve' appears 12971 times\n",
            "'extend' appears 9165 times\n",
            "'further research' appears 2162 times\n",
            "'challenge' appears 1163 times\n",
            "'limitation' appears 816 times\n",
            "'need' appears 489 times\n",
            "'require' appears 313 times\n",
            "'future work' appears 202 times\n",
            "'remain' appears 70 times\n",
            "'unexplored' appears 5 times\n",
            "'not addressed' appears 1 times\n",
            "\n",
            "Top research fields:\n",
            "field\n",
            "Computational Theory           16103\n",
            "Computer Vision                14786\n",
            "Natural Language Processing    14007\n",
            "Artificial Intelligence        11930\n",
            "Human Computer Interaction      6893\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Analyzing future work patterns\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Combining all future works text\n",
        "all_future_works = df_clean['future_work_cleaned'].fillna('')\n",
        "future_text = ' '.join(all_future_works)\n",
        "\n",
        "# Some common research gap indicators\n",
        "gap_indicators = [\n",
        "    'improve', 'enhance', 'extend', 'future work', 'limitation',\n",
        "    'challenge', 'unexplored', 'further research', 'investigate',\n",
        "    'not addressed', 'remain', 'need', 'require'\n",
        "]\n",
        "\n",
        "# Counting appearances of each indicator and storing in dictionary)\n",
        "gap_counts = {}\n",
        "for indicator in gap_indicators:\n",
        "    gap_counts[indicator] = future_text.count(indicator)\n",
        "\n",
        "# Viewing  results in descending order\n",
        "for word, count in sorted(gap_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"'{word}' appears {count} times\")\n",
        "\n",
        "# Grouping by research field to find field-specific gaps\n",
        "field_gaps = df_clean.groupby('field')['Future Works'].apply(\n",
        "    lambda x: ' '.join(x.fillna(''))\n",
        ").to_dict()\n",
        "\n",
        "# Showing top fields with most papers\n",
        "top_fields = df_clean['field'].value_counts().head(5)\n",
        "print(\"\\nTop research fields:\")\n",
        "print(top_fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rDwtXOs8-1UR",
      "metadata": {
        "id": "rDwtXOs8-1UR"
      },
      "outputs": [],
      "source": [
        "# Some explicit gap-indicating phrases\n",
        "explicit_gap_phrases = [\n",
        "    # Direct limitations\n",
        "    'limitation', 'limited', 'restrict', 'restricted', 'challenge', 'challenging',\n",
        "    'difficult', 'difficulty', 'problem', 'issue', 'drawback', 'shortcoming',\n",
        "\n",
        "    # Future work indicators\n",
        "    'future work', 'future research', 'further research', 'further investigation',\n",
        "    'future direction', 'next step', 'remain', 'remains', 'still need',\n",
        "\n",
        "    # Negation patterns\n",
        "    'not addressed', 'not considered', 'not explored', 'not investigated',\n",
        "    'lack', 'lacking', 'absent', 'missing', 'incomplete', 'insufficient',\n",
        "\n",
        "    # Improvement indicators\n",
        "    'could be improved', 'need improvement', 'should be enhanced',\n",
        "    'require', 'requires', 'needs', 'demand', 'necessitate'\n",
        "]\n",
        "\n",
        "# Searching for these phrases in gap corpus\n",
        "def extract_gap_sentences(text, gap_phrases):\n",
        "    sentences = text.split('.')\n",
        "    gap_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence = sentence.strip().lower()\n",
        "        for phrase in gap_phrases:\n",
        "            if phrase in sentence:\n",
        "                gap_sentences.append(sentence)\n",
        "                break\n",
        "\n",
        "    return gap_sentences\n",
        "\n",
        "df_gap_ready['gap_sentences'] = df_gap_ready['gap_corpus'].apply(\n",
        "    lambda x: extract_gap_sentences(x, explicit_gap_phrases)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yuEA_kpc_aLm",
      "metadata": {
        "id": "yuEA_kpc_aLm"
      },
      "outputs": [],
      "source": [
        "# Looking for specific patterns that indicate research gaps\n",
        "import re\n",
        "\n",
        "def extract_contextual_gaps(text):\n",
        "    gap_contexts = []\n",
        "\n",
        "    # Pattern 1: \"However, [gap description]\"\n",
        "    however_pattern = r'however,\\s*([^.]*)'\n",
        "    however_matches = re.findall(however_pattern, text.lower())\n",
        "    gap_contexts.extend([f\"however_gap: {match}\" for match in however_matches])\n",
        "\n",
        "    # Pattern 2: \"Future work [specific direction]\"\n",
        "    future_pattern = r'future work[^.]*([^.]*)'\n",
        "    future_matches = re.findall(future_pattern, text.lower())\n",
        "    gap_contexts.extend([f\"future_gap: {match}\" for match in future_matches])\n",
        "\n",
        "    # Pattern 3: \"Our method fails/struggles when...\"\n",
        "    failure_pattern = r'(fails?|struggles?|cannot|unable)[^.]*'\n",
        "    failure_matches = re.findall(failure_pattern, text.lower())\n",
        "    gap_contexts.extend([f\"limitation_gap: {match}\" for match in failure_matches])\n",
        "\n",
        "    return gap_contexts\n",
        "\n",
        "df_gap_ready['contextual_gaps'] = df_gap_ready['gap_corpus'].apply(extract_contextual_gaps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l-33fehk8udv",
      "metadata": {
        "id": "l-33fehk8udv"
      },
      "outputs": [],
      "source": [
        "# Extracting field-specific gap patterns\n",
        "def get_field_specific_gaps(field_name, corpus_text):\n",
        "\n",
        "    # AI/ML specific gaps\n",
        "    ai_gaps = ['overfitting', 'underfitting', 'generalization', 'scalability',\n",
        "               'interpretability', 'bias', 'fairness', 'robustness']\n",
        "\n",
        "    # Computer Vision gaps\n",
        "    cv_gaps = ['occlusion', 'lighting conditions', 'viewpoint variation',\n",
        "               'real-time processing', 'annotation cost']\n",
        "\n",
        "    # NLP gaps\n",
        "    nlp_gaps = ['out-of-vocabulary', 'cross-lingual', 'low-resource',\n",
        "                'domain adaptation', 'context understanding']\n",
        "\n",
        "    field_gap_dict = {\n",
        "        'artificial intelligence': ai_gaps,\n",
        "        'computer vision': cv_gaps,\n",
        "        'natural language processing': nlp_gaps\n",
        "    }\n",
        "\n",
        "    found_gaps = []\n",
        "    if field_name.lower() in field_gap_dict:\n",
        "        for gap_term in field_gap_dict[field_name.lower()]:\n",
        "            if gap_term in corpus_text.lower():\n",
        "                found_gaps.append(gap_term)\n",
        "\n",
        "    return found_gaps\n",
        "\n",
        "df_gap_ready['domain_gaps'] = df_gap_ready.apply(\n",
        "    lambda row: get_field_specific_gaps(row['field'], row['gap_corpus']),\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UlOxabaE8wgH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlOxabaE8wgH",
        "outputId": "4d4ad910-a80c-4082-f233-8997384d41fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GAP EXTRACTION SUMMARY:\n",
            "Total papers: 80318\n",
            "Papers with gap sentences: 63537\n",
            "Papers with contextual gaps: 40220\n",
            "Papers with domain gaps: 7651\n",
            "Average gap sentences per paper: 2.8\n",
            "Average contextual gaps per paper: 1.0\n",
            "Average domain gaps per paper: 0.1\n"
          ]
        }
      ],
      "source": [
        "# Checking extraction results\n",
        "print(\"GAP EXTRACTION SUMMARY:\")\n",
        "print(f\"Total papers: {len(df_gap_ready)}\")\n",
        "print(f\"Papers with gap sentences: {(df_gap_ready['gap_sentences'].str.len() > 0).sum()}\")\n",
        "print(f\"Papers with contextual gaps: {(df_gap_ready['contextual_gaps'].str.len() > 0).sum()}\")\n",
        "print(f\"Papers with domain gaps: {(df_gap_ready['domain_gaps'].str.len() > 0).sum()}\")\n",
        "\n",
        "# Average gaps per paper\n",
        "print(f\"Average gap sentences per paper: {df_gap_ready['gap_sentences'].str.len().mean():.1f}\")\n",
        "print(f\"Average contextual gaps per paper: {df_gap_ready['contextual_gaps'].str.len().mean():.1f}\")\n",
        "print(f\"Average domain gaps per paper: {df_gap_ready['domain_gaps'].str.len().mean():.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cS1Ben_K_Lra",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS1Ben_K_Lra",
        "outputId": "1576584a-bb11-4ef1-dcaf-d7ca543a0be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SAMPLE GAP EXTRACTIONS:\n",
            "\n",
            "--- Paper 1: Generalizable Representation Learning for Mixture Domain Face Anti-Spoofing... ---\n",
            "Field: Artificial Intelligence\n",
            "Gap Sentences:\n",
            "  â¢ to overcome the limitation, we propose domain dynamic adjustment meta-learning (d$^2$am) without usi...\n",
            "  â¢ hence, we think it is important to simulate difficult and abundant domain shift scenarios for meta-l...\n",
            "  â¢ a drlm and mmdbased regularization are designed for better dynamic adjustment to simulate more diffi...\n",
            "Contextual Gaps:\n",
            "  â¢ however_gap: in real-world applications, the collected dataset always contains mixture domains, wher...\n",
            "Domain Gaps:\n",
            "  â¢ ['generalization', 'interpretability', 'robustness']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Paper 2: Coordinating Human and Agent Behavior in Collective-Risk Scenarios... ---\n",
            "Field: Artificial Intelligence\n",
            "Gap Sentences:\n",
            "Contextual Gaps:\n",
            "  â¢ however_gap: only the ones that used it to predict future outcomes (anticipative agents) were able t...\n",
            "Domain Gaps:\n",
            "  â¢ []\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Paper 3: Adverse Drug Reaction Prediction with Symbolic Latent Dirichlet Allocation... ---\n",
            "Field: Artificial Intelligence\n",
            "Gap Sentences:\n",
            "  â¢ traditional preclinical in vitro safety profiling and clinical safety trials are restricted in terms...\n",
            "  â¢ ', 'addressing issues related to false-negative predictions...\n",
            "  â¢ furthermore, we analyze the positive and false negative predictions based on a sample drug and the r...\n",
            "Contextual Gaps:\n",
            "Domain Gaps:\n",
            "  â¢ ['interpretability']\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Paper 4: TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL... ---\n",
            "Field: Artificial Intelligence\n",
            "Gap Sentences:\n",
            "  â¢ temple gains sample efficiency by extracting similarities of the transition dynamics across tasks ev...\n",
            "Contextual Gaps:\n",
            "  â¢ however_gap: these methods may be inefficient when the underlying models or optimal policies are sub...\n",
            "Domain Gaps:\n",
            "  â¢ []\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Paper 5: Scalable Recollections for Continual Lifelong Learning... ---\n",
            "Field: Artificial Intelligence\n",
            "Gap Sentences:\n",
            "  â¢ perhaps the most difficult of these settings is that of continual lifelong learning, where the model...\n",
            "  â¢ recent techniques have focused their efforts primarily on the first two capabilities while questions...\n",
            "  â¢ in this paper, we consider the problem of efficient and effective storage of experiences over very l...\n",
            "Contextual Gaps:\n",
            "Domain Gaps:\n",
            "  â¢ ['scalability']\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Reviewing sample extractions\n",
        "print(\"\\nSAMPLE GAP EXTRACTIONS:\")\n",
        "for i in range(5):\n",
        "    paper = df_gap_ready.iloc[i]\n",
        "    print(f\"\\n--- Paper {i+1}: {paper['title'][:80]}... ---\")\n",
        "    print(f\"Field: {paper['field']}\")\n",
        "\n",
        "    print(\"Gap Sentences:\")\n",
        "    for sentence in paper['gap_sentences'][:3]:\n",
        "        print(f\"  â¢ {sentence[:100]}...\")\n",
        "\n",
        "    print(\"Contextual Gaps:\")\n",
        "    for gap in paper['contextual_gaps'][:2]:\n",
        "        print(f\"  â¢ {gap[:100]}...\")\n",
        "\n",
        "    print(\"Domain Gaps:\")\n",
        "    print(f\"  â¢ {paper['domain_gaps']}\")\n",
        "    print(\"-\" * 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0C-qJZRe_ydD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C-qJZRe_ydD",
        "outputId": "56bc4faa-f1b5-484f-80f4-f7b1edac6cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "GAPS BY RESEARCH FIELD\n",
            "                                    gap_sentences  contextual_gaps  \\\n",
            "field                                                                \n",
            "Computer Vision                             46465            15990   \n",
            "Computational Theory                        45513            12776   \n",
            "Artificial Intelligence                     42882            13583   \n",
            "Natural Language Processing                 33287            13412   \n",
            "Human Computer Interaction                  17673             6010   \n",
            "Computing in Biomedical Fields              11170             3802   \n",
            "Computer Networks & Communications           9513             3387   \n",
            "Software Engineering                         8108             3268   \n",
            "Graphics and Computer-Aided Design           5430             2008   \n",
            "Computer Hardware & Architecture             3426             1372   \n",
            "\n",
            "                                    domain_gaps  \n",
            "field                                            \n",
            "Computer Vision                            1255  \n",
            "Computational Theory                          0  \n",
            "Artificial Intelligence                    6349  \n",
            "Natural Language Processing                1835  \n",
            "Human Computer Interaction                    0  \n",
            "Computing in Biomedical Fields                0  \n",
            "Computer Networks & Communications            0  \n",
            "Software Engineering                          0  \n",
            "Graphics and Computer-Aided Design            0  \n",
            "Computer Hardware & Architecture              0  \n"
          ]
        }
      ],
      "source": [
        "# Checking gaps by research field\n",
        "gap_by_field = df_gap_ready.groupby('field').agg({\n",
        "    'gap_sentences': lambda x: sum(len(sentences) for sentences in x),\n",
        "    'contextual_gaps': lambda x: sum(len(gaps) for gaps in x),\n",
        "    'domain_gaps': lambda x: sum(len(gaps) for gaps in x)\n",
        "}).sort_values('gap_sentences', ascending=False)\n",
        "\n",
        "print(\"\\nGAPS BY RESEARCH FIELD\")\n",
        "print(gap_by_field.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MqZthaoE_3wI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqZthaoE_3wI",
        "outputId": "1338056f-10b9-49da-d4a3-f4a729a54189"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MOST COMMON GAP PATTERNS\n",
            "Total gap sentences extracted: 228191\n",
            "Total contextual gaps extracted: 77439\n",
            "\n",
            "Sample gap sentences:\n",
            "1. to overcome the limitation, we propose domain dynamic adjustment meta-learning (d$^2$am) without using domain labels, wh...\n",
            "2. hence, we think it is important to simulate difficult and abundant domain shift scenarios for meta-learning...\n",
            "3. a drlm and mmdbased regularization are designed for better dynamic adjustment to simulate more difficult and abundant do...\n",
            "4. traditional preclinical in vitro safety profiling and clinical safety trials are restricted in terms of small scale, lon...\n",
            "5. ', 'addressing issues related to false-negative predictions...\n",
            "6. furthermore, we analyze the positive and false negative predictions based on a sample drug and the results point to some...\n",
            "7. temple gains sample efficiency by extracting similarities of the transition dynamics across tasks even when their underl...\n",
            "8. perhaps the most difficult of these settings is that of continual lifelong learning, where the model must learn online o...\n",
            "9. recent techniques have focused their efforts primarily on the first two capabilities while questions of efficiency remai...\n",
            "10. in this paper, we consider the problem of efficient and effective storage of experiences over very large time-frames...\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Collecting all gap sentences\n",
        "all_gap_sentences = []\n",
        "for sentences in df_gap_ready['gap_sentences']:\n",
        "    all_gap_sentences.extend(sentences)\n",
        "\n",
        "# Collecting all contextual gaps\n",
        "all_contextual_gaps = []\n",
        "for gaps in df_gap_ready['contextual_gaps']:\n",
        "    all_contextual_gaps.extend(gaps)\n",
        "\n",
        "print(f\"\\nMOST COMMON GAP PATTERNS\")\n",
        "print(f\"Total gap sentences extracted: {len(all_gap_sentences)}\")\n",
        "print(f\"Total contextual gaps extracted: {len(all_contextual_gaps)}\")\n",
        "\n",
        "# Sampling the most common patterns\n",
        "print(\"\\nSample gap sentences:\")\n",
        "for i, sentence in enumerate(all_gap_sentences[:10]):\n",
        "    print(f\"{i+1}. {sentence[:120]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eBM3qI56_8dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBM3qI56_8dd",
        "outputId": "7ca8fa58-b087-49d3-9036-9ed973ae8a90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "QUALITY METRICS\n",
            "Papers with NO gaps extracted: 11530\n",
            "Papers with gaps extracted: 68788\n",
            "Success rate: 85.6%\n",
            "Papers with rich gap info: 35684\n"
          ]
        }
      ],
      "source": [
        "# Quality metrics\n",
        "empty_gaps = (df_gap_ready['gap_sentences'].str.len() == 0) & \\\n",
        "            (df_gap_ready['contextual_gaps'].str.len() == 0) & \\\n",
        "            (df_gap_ready['domain_gaps'].str.len() == 0)\n",
        "\n",
        "print(f\"\\nQUALITY METRICS\")\n",
        "print(f\"Papers with NO gaps extracted: {empty_gaps.sum()}\")\n",
        "print(f\"Papers with gaps extracted: {(~empty_gaps).sum()}\")\n",
        "print(f\"Success rate: {(~empty_gaps).mean()*100:.1f}%\")\n",
        "\n",
        "# Papers with rich gap information\n",
        "rich_papers = (df_gap_ready['gap_sentences'].str.len() > 0) & \\\n",
        "             (df_gap_ready['contextual_gaps'].str.len() > 0)\n",
        "print(f\"Papers with rich gap info: {rich_papers.sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zM1ZI2oRACdV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM1ZI2oRACdV",
        "outputId": "ee3f0e8e-bb06-4828-eaa2-568de395dba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data saved successfully!\n",
            "\n",
            "Files created:\n",
            "\n",
            "research_gaps_dataset.csv (main dataset)\n",
            "\n",
            "research_gaps_dataset.pkl (faster loading)\n",
            "\n",
            "gap_extraction_summary.json (summary stats)\n"
          ]
        }
      ],
      "source": [
        "# Saving the main dataset\n",
        "df_gap_ready.to_csv('research_gaps_dataset.csv', index=False)\n",
        "df_gap_ready.to_pickle('research_gaps_dataset.pkl')\n",
        "\n",
        "# Saving gap extractions summary for quick reference\n",
        "gap_summary = {\n",
        "    'total_papers': len(df_gap_ready),\n",
        "    'total_gap_sentences': len(all_gap_sentences),\n",
        "    'total_contextual_gaps': len(all_contextual_gaps),\n",
        "    'success_rate': (~empty_gaps).mean(),\n",
        "    'top_fields': gap_by_field.head(5).to_dict()\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('gap_extraction_summary.json', 'w') as f:\n",
        "    json.dump(gap_summary, f, indent=2)\n",
        "\n",
        "print(\"Data saved successfully!\")\n",
        "print(\"\\nFiles created:\")\n",
        "print(\"\\nresearch_gaps_dataset.csv (main dataset)\")\n",
        "print(\"\\nresearch_gaps_dataset.pkl (faster loading)\")\n",
        "print(\"\\ngap_extraction_summary.json (summary stats)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e710ed07",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
